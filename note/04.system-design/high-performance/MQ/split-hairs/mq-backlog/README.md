# MQ消息积压

## 一、消息积压的本质与影响
消息积压（Message Backlog）指消息生产者发送消息的速度持续超过消费者处理能力，导致未处理消息在队列中堆积。其核心矛盾是**生产速率 > 消费速率**，若未及时干预，积压会像滚雪球般恶化，引发以下后果：
- **系统性能下降**：资源（CPU、内存、磁盘I/O）被积压消息占用，导致新消息处理延迟。
- **业务风险**：核心业务（如订单支付）可能因消息延迟而失败，影响用户体验和收入。
- **数据丢失风险**：队列容量超限或消息过期时，数据可能永久丢失。
- **系统崩溃**：极端情况下，积压导致MQ服务不可用，引发服务雪崩。

## 二、消息积压的常见原因
### 1. 消费端问题（占比80%）
- **性能瓶颈**：
    - 消费逻辑复杂（如频繁调用外部接口、慢SQL查询）。
    - 线程池配置不合理（如线程数不足或阻塞）。
    - 消费者假死（如GC停顿、死锁、死循环）。
- **配置缺陷**：
    - 顺序消费中单条消息卡住，阻塞整个队列。
    - 广播模式重复处理导致效率低下。
- **故障**：
    - 消费者宕机或网络中断，无法拉取消息。

### 2. 生产端问题（占比10%）
- **流量激增**：
    - 突发大流量（如秒杀活动、定时任务集中触发）。
    - 补偿机制重发导致消息重复。
- **发送失控**：
    - 生产者线程池配置错误，发送速率远超预期。

### 3. Broker端问题（占比10%）
- **资源瓶颈**：
    - 磁盘I/O性能不足（如PageCache刷盘慢）。
    - 网络带宽不足或分区。
- **队列设计缺陷**：
    - 队列数量不足，无法充分利用消费者并行能力。

## 三、解决方案：紧急止血与长期优化
### 1. 紧急止血（临时方案）
**目标**：快速恢复消费速度，防止积压进一步恶化。
- **扩容消费者实例**：
    - 通过Kubernetes（HPA）或脚本动态增加消费者Pod。
    - **关键点**：确保消费者实例数 ≤ 队列数（如RocketMQ中，每个队列同一时间只能被一个消费者消费）。
- **开启批量消费**：
    - 修改消费者配置，一次处理多条消息（需实现幂等性）。
    - **示例**：`consumer.setConsumeMessageBatchMaxSize(10);`（RocketMQ）。
- **暂停非核心业务**：
    - 降低非关键业务消费者线程数，释放资源给核心业务。
- **消息转储**：
    - 将积压消息临时存储到数据库或文件系统，后续通过离线任务处理。

### 2. 长期优化（架构治根）
**目标**：提升系统吞吐量，预防积压复发。
- **优化消费逻辑**：
    - **异步化**：将耗时操作（如数据库写入、RPC调用）改为异步处理。
    - **批量操作**：合并数据库写入（如批量INSERT）减少I/O开销。
    - **代码优化**：使用Arthas或jstack定位慢代码段，优化线程池配置。
- **水平扩展**：
    - **消费者扩容**：根据业务峰值流量预估队列数，确保消费者实例数 ≥ 队列数 / 单实例线程数。
    - **队列扩容**：动态增加Topic的队列数（需确保Broker允许自动创建Topic）。
        - **示例**：`sh mqadmin updateTopic -n namesrv-ip:9876 -t order_topic -w 8 -r 8`（RocketMQ）。
- **限流与降级**：
    - **生产端限流**：通过令牌桶或漏桶算法控制发送速率。
        - **示例**：`producer.setSendMsgTimeout(5000); producer.setRetryTimesWhenSendFailed(2);`（RocketMQ）。
    - **服务降级**：在高峰期暂停非核心服务消息生产，或直接丢弃不重要消息。
- **优先级队列**：
    - 将核心业务消息设为高优先级，确保优先处理。
    - **示例**：RocketMQ支持18个延迟级别，可配置高优先级消息延迟更低。
- **死信队列管理**：
    - 将无法正常处理的消息转移到死信队列，避免阻塞主流队列。
    - 配置死信队列监控与告警，及时处理异常消息。

### 3. 监控与告警
**目标**：提前发现积压风险，实现快速响应。
- **关键指标监控**：
    - 待消费消息数：持续增长说明消费能力不足。
    - 最后一次消费时间：间隔过长代表消费者可能卡住。
    - 消费速度/生产速度比值：比值过低（如远小于1）说明积压风险高。
- **工具推荐**：
    - **Prometheus + Grafana**：采集MQ指标（如队列长度、消费延迟）并可视化。
    - **RocketMQ Dashboard**：实时查看消费者分组、队列分配、消费进度。
    - **ELK日志分析**：通过日志定位消费慢的原因（如慢SQL、外部接口超时）。
- **阈值告警**：
    - 设置待消费数超过1万、消费延迟超过5分钟等阈值，触发邮件/钉钉/微信告警。

## 四、实战案例：电商大促消息积压处理
**场景**：电商大促期间，订单系统消息积压20万条，导致用户支付后订单状态更新延迟。
**处理步骤**：
1. **紧急止血**：
    - 动态扩容消费者实例至50个（原10个），充分利用队列资源。
    - 开启批量消费，每次处理10条消息。
    - 暂停非核心业务（如日志采集）消费者，释放资源。
2. **长期优化**：
    - 优化消费逻辑：将订单状态更新操作改为异步，减少数据库锁竞争。
    - 水平扩展：增加Topic队列数至100个，确保消费者实例数 ≥ 队列数 / 单实例线程数（20）。
    - 限流：对生产者进行限流，确保发送速率不超过消费者处理能力。
3. **监控加固**：
    - 配置Prometheus + Grafana监控待消费数、消费延迟等指标。
    - 设置阈值告警，待消费数超过5000条时触发告警。

## 五、预防性建议
1. **压测与演练**：
    - 使用JMeter或LoadRunner模拟峰值流量，验证消费者处理能力。
    - 定期进行混沌工程演练，注入消息处理延迟，测试系统容错性。
2. **SLA设计**：
    - 定义消息类型积压阈值与处理策略（如支付订单 > 1000条自动扩容 + 短信告警）。
3. **灰度与回滚**：
    - 消费逻辑变更时，新版本消费者订阅新Topic，逐步切流（如10% → 50% → 100%）。
    - 异常时秒级切回旧版本，避免影响生产环境。