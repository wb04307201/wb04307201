# 《大模型与算力

> - 什么是大模型？
> - 为什么AI要用英伟达显卡？

## 大模型基础
- **AI层级关系**：
    - AI机器学习
    - 深度学习
    - 生成式AI（AIGC）
    - 大语言模型（LLM）
- **术语解释**：
    - **AIGC**：AI生成内容，涵盖文本、图片、视频等。
    - **生成式AI**：能完成AIGC任务的AI模型。

## 机器学习与神经网络
- **机器学习类型**：
    - **监督学习**：利用有标记的训练集，应用于分类、回归。
    - **无监督学习**：利用无标记的训练集，应用于聚类。
- **神经网络基础**：
    - **参数**：神经元间的关系表示，如线性函数系数。
    - **超参数**：控制模型训练，如迭代步数、激活函数、学习率。
    - **激活函数**：将线性函数转化为非线性，提高表达能力。

## 模型评估与训练
- **数据集划分**：
    - **训练集**：用于模型训练。
    - **验证集**：用于模型验证。
    - **测试集**：用于评价模型泛化能力。
- **模型问题**：
    - **欠拟合**：模型未找到合适数据关系。
    - **过拟合**：模型过度学习训练集数据，泛化能力差。
- **训练过程**：
    - **无监督预训练**：利用海量文本数据。
    - **监督微调（SFT）**：利用标记好的对话数据。
    - **强化学习**：引入奖励模型评价回答。

## Token与Transformer
- **Token**：分词器拆分的有意义文本单元，词向量表示。
- **词嵌入**：将自然语言转化为token的过程。
- **Transformer结构**：
    - **编码器**：将自然语言转化为词向量。
    - **解码器**：融合编码器输入和解码器输出，预测完整内容。
    - **自注意力机制**：提高模型理解能力。
    - **多头注意力机制**：从不同角度理解句子。

## 大模型生成与幻觉控制
- **生成回答流程**：
    - **Temperature**：影响预测概率分布。
    - **Top-k与Top-p**：取概率分布的前k个或总和为p的前n个值重新计算概率。
- **幻觉问题**：
    - **定义**：上下文矛盾、与提示词不一致、与事实矛盾、荒谬回复。
    - **原因**：数据质量差、模型过拟合、提示词不明确、生成过程随机。
    - **控制方法**：引入RAG（检索增强）技术。

## 大模型规格与显存
- **规格与显存关系**：
    - **参数类型**：float（32位）、half/BF16（16位）、int8（8位）、int4（4位）。
    - **推理显存占用**：参数量*参数类型。
    - **微调显存需求**：全量微调需3~4倍推理显存，高效微调（PEFT）需约1.1倍。

## 大模型压缩方案
- **混合专家（MoE）**：前置router识别问题分类到Expert模型。
- **量化**：降低参数存储精度，减少内存空间。
- **蒸馏**：使用小规模模型模仿大模型行为。

## 大模型落地场景
- **RAG检索增强**：外挂知识库，检索答案并融合问题回答。
- **AI Agent**：定义大模型技能，拆解问题并规划解决步骤。

## 大模型涌现现象
- **定义**：系统量变导致行为质变。
- **表现**：
    - **In Context Learning**：少量提示词下表现更好。
    - **Chain of Thought**：分步思考解决问题表现更好。

## 算力基础
- **算力分类**：
    - **基础算力**：CPU计算能力。
    - **智能算力**：GPU、FPGA、ASIC芯片计算能力。
    - **超算算力**：超算集群计算能力。
- **衡量单位**：FLOPS（每秒浮点运算次数），MIPS、DMIPS、OPS、Hash/s等。

## GPU与CUDA
- **GPU规格参数**：标准算力、Tenser core加速后算力、稀疏矩阵加速后算力。
- **CUDA**：英伟达开发的编程平台，用于加速GPU和CPU间计算，简化AI调用GPU过程。

## 总结
- **大模型**：根据上文预测下一个字的人工智能模型。
- **英伟达显卡**：AI选择英伟达显卡主要因为CUDA生态。