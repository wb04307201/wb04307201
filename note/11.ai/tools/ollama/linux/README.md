### **Qwen3 模型服务器配置建议表 (Ollama GPU 运行)**

在规划部署方案前，请根据您希望运行的模型规模，参考以下硬件配置建议。

| 模型名称           | 大小    | 最大 token 数 | **推荐显存 (VRAM)** | **推荐系统内存 (RAM)** | **推荐 CPU (最低)** | **推荐存储 (SSD)** | 备注/说明                                                                                                   |
|:---------------|:------|:-----------|:----------------|:-----------------|:----------------|:---------------|:--------------------------------------------------------------------------------------------------------|
| **qwen3:0.6b** | 523MB | 40K        | **≥ 4GB**       | **≥ 8GB**        | 4 核             | ≥ 100GB        | 入门级。几乎所有现代独立显卡都能轻松运行，甚至一些高端核显也可以。                                                                       |
| **qwen3:1.7b** | 1.4GB | 40K        | **≥ 6GB**       | **≥ 16GB**       | 4 核             | ≥ 100GB        | 消费级显卡的甜点区，如 NVIDIA RTX 3060 (8GB/12GB) 或更高。                                                             |
| **qwen3:4b**   | 2.5GB | 256K       | **≥ 10GB**      | **≥ 16GB**       | 6 核             | ≥ 100GB        | 256K 上下文会显著增加 KV Cache 占用。建议使用显存 12GB 以上的显卡，如 RTX 3060 12GB/3080。                                       |
| **qwen3:8b**   | 5.2GB | 40K        | **≥ 12GB**      | **≥ 16GB**       | 8 核             | ≥ 150GB        | 消费级显卡的高端测试。RTX 3060 12GB 刚好满足最低要求，RTX 3090/4090 (24GB) 运行会非常流畅。                                         |
| **qwen3:14b**  | 9.3GB | 40K        | **≥ 24GB**      | **≥ 32GB**       | 8 核             | ≥ 200GB        | **专业卡入门**。消费级显卡中只有 RTX 3090/4090 (24GB) 可以勉强运行。强烈推荐 24GB 显存以上的专业卡，如 A5000。                              |
| **qwen3:30b**  | 19GB  | 256K       | **≥ 48GB**      | **≥ 64GB**       | 12 核            | ≥ 250GB        | **高端专业卡/数据中心级**。单张 RTX 4090 (24GB) 无法独立运行。需要 A100 (40GB/80GB) 或 H100 (80GB) 等，或使用两张显卡。256K 上下文对显存是巨大挑战。 |
| **qwen3:32b**  | 20GB  | 40K        | **≥ 48GB**      | **≥ 64GB**       | 12 核            | ≥ 250GB        | 与 30b 类似，需要高端数据中心 GPU。单张 40GB 的 A100 可以运行，但非常紧凑。                                                        |
| **qwen3:235b** | 142GB | 256K       | **≥ 480GB**     | **≥ 512GB**      | 16 核+           | ≥ 1TB          | **企业/研究级**。单张 GPU 无法运行。必须使用多 GPU 服务器，例如 **4 x NVIDIA H100 (80GB)** 才能加载模型权重。这是典型的大规模语言模型部署场景。           |

---

### **Qwen3 大模型 Linux 服务器部署方案简述**

本方案旨在指导您在配备 NVIDIA GPU 的 Linux 服务器上，使用 Ollama 快速部署并运行 Qwen3 系列大模型。Ollama 极大地简化了模型管理和服务化的过程。

#### **第一步：环境准备 (Prerequisites)**

1.  **操作系统**: 推荐使用主流的 Linux 发行版，如 Ubuntu 22.04 LTS 或 CentOS 7+。
2.  **NVIDIA 驱动**: 确保服务器已安装最新的 NVIDIA 显卡驱动。您可以通过运行 `nvidia-smi` 命令来检查驱动状态和 GPU 信息。
3.  **NVIDIA Container Toolkit (推荐)**: 为了让 Docker 等容器工具能更好地与 GPU 交互，建议安装 NVIDIA Container Toolkit。虽然 Ollama 原生安装不强制要求，但这是一个良好的实践。
    ```bash
    # Ubuntu 示例
    sudo apt-get update
    sudo apt-get install -y nvidia-container-toolkit
    sudo systemctl restart docker
    ```

#### **第二步：安装 Ollama**

Ollama 提供了一个便捷的一键安装脚本，它会自动检测您的系统并安装 Ollama 服务。

```bash
# 下载并执行官方安装脚本
curl -fsSL https://ollama.com/install.sh | sh
```

安装完成后，Ollama 会作为一个系统服务（systemd）在后台运行。您可以使用以下命令来管理它：

```bash
# 启动 Ollama 服务
sudo systemctl start ollama

# 设置开机自启
sudo systemctl enable ollama

# 查看服务状态
sudo systemctl status ollama
```

#### **第三步：选择并运行 Qwen3 模型**

这是最核心的步骤。Ollama 通过一个简单的命令来下载并运行模型。模型将从 Ollama 的模型库中拉取。

1.  **列出可用模型**: 您可以先搜索 Qwen3 相关的模型。
    ```bash
    ollama list
    ```

2.  **运行模型**: 使用 `ollama run` 命令。Ollama 会自动下载模型（如果本地不存在），加载到 GPU 显存，并启动一个交互式聊天会话。

    **示例：运行 qwen3:4b 模型**
    ```bash
    ollama run qwen3:4b
    ```

    **示例：运行 qwen3:14b 模型 (需要显存 ≥ 24GB)**
    ```bash
    ollama run qwen3:14b
    ```

    首次运行时，您会看到模型文件的下载进度条。下载完成后，会出现 `>>>` 提示符，您可以直接开始与模型对话。

    **重要提示**:
    *   **GPU 验证**: 在模型加载时，您可以打开另一个终端窗口，运行 `nvidia-smi`。如果看到有 Python 或 ollama 进程占用了大量的 GPU 显存，说明模型已成功加载到 GPU 上。
    *   **显存不足**: 如果您的显存不足以容纳整个模型，Ollama 会自动将部分层卸载到 CPU 内存中。这会导致运行速度显著变慢，但能让您在显存较小的设备上运行更大的模型。

#### **第四步：验证与交互**

当您执行 `ollama run` 后，会直接进入一个命令行交互界面。

```
>>> Send a message (/? for help)
```

您可以输入任何问题，例如：
`你好，请介绍一下你自己。`

按 `Ctrl+D` 或输入 `/bye` 可以退出交互式会话。模型会保留在显存中下次运行会更快。

#### **第五步：通过 API 进行调用 (服务化)**

Ollama 的一大优势是它会自动启动一个 REST API 服务（默认在 `http://localhost:11434`），方便应用程序集成。

您可以使用 `curl` 或任何编程语言来调用这个 API。

**使用 `curl` 发送请求的示例**：

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:4b",
  "prompt": "为什么天空是蓝色的？",
  "stream": false
}'
```

**参数说明**:
*   `model`: 指定要使用的模型名称。
*   `prompt`: 您的提示词。
*   `stream`: `false` 表示等待模型完全生成后再返回全部结果；`true` 表示以流式方式逐字返回结果。

至此，您已成功在 Linux 服务器上部署了 Qwen3 大模型，并可以通过命令行或 API 的方式与其进行交互。您可以根据业务需求，将此 API 集成到您的应用程序中。